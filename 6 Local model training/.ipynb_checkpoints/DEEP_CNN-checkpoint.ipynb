{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "colab_type": "code",
    "id": "1YLRO_gO6hgh",
    "outputId": "b2bf8436-a710-42ee-9f03-07428ff2d7e3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
      "\n",
      "Enter your authorization code:\n",
      "··········\n",
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "xnMjT1DF6_zn",
    "outputId": "4c937375-0d25-4dc0-db93-3dfd0d044b39"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/drive/My Drive\n"
     ]
    }
   ],
   "source": [
    "cd ./drive/My\\ Drive/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "colab_type": "code",
    "id": "fHfp_e_G4Azp",
    "outputId": "8b0a190a-a3ef-4e6e-966a-f9c33269593a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "`%tensorflow_version` only switches the major version: 1.x or 2.x.\n",
      "You set: `1.4`. This will be interpreted as: `1.x`.\n",
      "\n",
      "\n",
      "TensorFlow 1.x selected.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%tensorflow_version 1.4\n",
    "import os\n",
    "import argparse\n",
    "import model_training.sentiment_dataset as sentiment_dataset\n",
    "import model_training.sentiment_model_cnn as sentiment_model_cnn\n",
    "import model_training.config_holder as config_holder\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "T-C5ajHF4Azw"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.layers import Conv1D\n",
    "from tensorflow.keras.layers import MaxPooling1D\n",
    "from tensorflow.keras.layers import GlobalMaxPool1D\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import Bidirectional\n",
    "from tensorflow.keras.layers import GRU\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "58nISPiE4Az1"
   },
   "outputs": [],
   "source": [
    "args = {'config_file': './model_training/training_config.json',\n",
    "       'num_epoch': 2, #5 if 25\n",
    "       'train': './model_training/data/train/',\n",
    "       'validation': './model_training/data/eval/',\n",
    "       'eval': './model_training/data/eval/'\n",
    "       }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 258
    },
    "colab_type": "code",
    "id": "aMgpcnmI4Az5",
    "outputId": "620a52d9-25f5-43a0-f257-f380601f7df5",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing for training...\n",
      "Fetching train data...\n",
      "train\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/data/util/random_seed.py:58: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From /content/drive/My Drive/model_training/sentiment_dataset.py:83: DatasetV1.make_one_shot_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_one_shot_iterator(dataset)`.\n",
      "Fetching validation data...\n",
      "eval\n",
      "Fetching eval data...\n",
      "eval\n"
     ]
    }
   ],
   "source": [
    "print(\"Preparing for training...\")\n",
    "\n",
    "training_config = config_holder.ConfigHolder(args['config_file']).config\n",
    "\n",
    "training_config[\"num_epoch\"] = args['num_epoch']\n",
    "\n",
    "train_dataset = sentiment_dataset.train_input_fn(args['train'], training_config)\n",
    "validation_dataset = sentiment_dataset.validation_input_fn(args['validation'], training_config)\n",
    "eval_dataset = sentiment_dataset.eval_input_fn(args['eval'], training_config)\n",
    "\n",
    "#train: 1,360,000\n",
    "#dev: 160,000\n",
    "#eval: 80,000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "ZqDUIRu74A0F",
    "outputId": "bc6b3741-a0d6-41c8-8aba-f3810200d0d7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1193515 word vectors.\n"
     ]
    }
   ],
   "source": [
    "embeddings_index = dict()\n",
    "# f = open('model_training/dictionary/glove.twitter.27B.25d.txt', encoding=\"utf-8\")\n",
    "f = open('model_training/dictionary/glove.twitter.27B.200d.txt', encoding=\"utf-8\")\n",
    "for line in f:\n",
    "\tvalues = line.split()\n",
    "\tword = values[0]\n",
    "\tcoefs = np.array(values[1:], dtype='float32')\n",
    "\tembeddings_index[word] = coefs\n",
    "f.close()\n",
    "print('Loaded %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "msaogivX4A0J"
   },
   "outputs": [],
   "source": [
    "vocab_size = len(embeddings_index.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mCtNZutj4A0P"
   },
   "outputs": [],
   "source": [
    "n = len(embeddings_index.keys())\n",
    "m = len(embeddings_index['the'])\n",
    "\n",
    "embedding_matrix = np.zeros((n,m))\n",
    "for index, key in zip(range(0, n), embeddings_index.keys()):\n",
    "    embedding_matrix[index] = embeddings_index[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "OZEMTfEr4A0Y",
    "outputId": "82964b9a-434d-4620-c471-bf6fd616a358"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1193515, 200)"
      ]
     },
     "execution_count": 10,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CilTg2c6qWUZ"
   },
   "source": [
    "##Model1: CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oj8Hl7e04A04"
   },
   "outputs": [],
   "source": [
    "# define model\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Embedding(vocab_size, 200, weights=[np.array(embedding_matrix)], input_length=100, trainable=True, name='embedding'))\n",
    "\n",
    "model.add(Conv1D(filters = 128, kernel_size = 5, strides = 1, padding = 'valid', activation = 'relu'))\n",
    "model.add(MaxPooling1D(2))\n",
    "\n",
    "model.add(Conv1D(filters = 256, kernel_size = 5, strides = 1, padding = 'valid', activation = 'relu'))\n",
    "model.add(MaxPooling1D(2))\n",
    "\n",
    "model.add(Conv1D(filters = 512, kernel_size = 5, strides = 1, padding = 'valid', activation = 'relu'))\n",
    "model.add(MaxPooling1D(2))\n",
    "\n",
    "model.add(Conv1D(filters = 1024, kernel_size = 5, strides = 1, padding = 'valid', activation = 'relu'))\n",
    "model.add(MaxPooling1D(2))\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(1024, activation = 'relu'))\n",
    "\n",
    "model.add(Dense(1, activation = 'sigmoid'))\n",
    "\n",
    "adam = Adam(lr=0.0005)\n",
    "\n",
    "model.compile(optimizer=adam, loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 561
    },
    "colab_type": "code",
    "id": "CfvIi4464A0-",
    "outputId": "b26946c3-924e-4816-900d-9fe5dc1b550e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 100, 200)          238703000 \n",
      "_________________________________________________________________\n",
      "conv1d (Conv1D)              (None, 96, 128)           128128    \n",
      "_________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D) (None, 48, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 44, 256)           164096    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 22, 256)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 18, 512)           655872    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 9, 512)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 5, 1024)           2622464   \n",
      "_________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1 (None, 2, 1024)           0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1024)              2098176   \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 1025      \n",
      "=================================================================\n",
      "Total params: 244,372,761\n",
      "Trainable params: 244,372,761\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "colab_type": "code",
    "id": "uaOAnZa0XQYw",
    "outputId": "1e0f2e35-3c58-4364-f6b2-6bd20a62c411"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1360 samples, validate on 1000 samples\n",
      "Epoch 1/2\n",
      "1360/1360 [==============================] - 580s 427ms/step - loss: 0.4583 - acc: 0.7804 - val_loss: 0.4381 - val_acc: 0.7928\n",
      "Epoch 2/2\n",
      "1360/1360 [==============================] - 559s 411ms/step - loss: 0.4169 - acc: 0.8066 - val_loss: 0.4323 - val_acc: 0.7974\n",
      "Test loss:0.4322528585791588\n",
      "Test accuracy:0.7974374890327454\n"
     ]
    }
   ],
   "source": [
    "model.fit(x = train_dataset[0], \n",
    "          y = train_dataset[1], \n",
    "          steps_per_epoch = train_dataset[2][\"num_batches\"],\n",
    "          epochs = training_config[\"num_epoch\"],\n",
    "          validation_data = (validation_dataset[0]['embedding_input'], validation_dataset[1]),\n",
    "          validation_steps = validation_dataset[2][\"num_batches\"])\n",
    "\n",
    "score = model.evaluate(\n",
    "eval_dataset[0], eval_dataset[1], steps=eval_dataset[2][\"num_batches\"], verbose=0)\n",
    "\n",
    "print(\"Test loss:{}\".format(score[0]))\n",
    "print(\"Test accuracy:{}\".format(score[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "0MWoomb4jyv_",
    "outputId": "10263a4b-a735-439c-93e6-70b897c572db"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss:0.4322528585791588\n",
      "Test accuracy:0.7974374890327454\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(\n",
    "eval_dataset[0], eval_dataset[1], steps=eval_dataset[2][\"num_batches\"], verbose=0)\n",
    "\n",
    "print(\"Test loss:{}\".format(score[0]))\n",
    "print(\"Test accuracy:{}\".format(score[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "ZZpjV-3qvynU",
    "outputId": "948fb544-66b2-4386-d049-913b337dd685"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: output_model_deep_cnn_final/sentiment_model.h5/1/assets\n"
     ]
    }
   ],
   "source": [
    "folder = 'output_model_deep_cnn_final'\n",
    "output = os.path.join(folder, \"sentiment_model.h5\")\n",
    "tf.saved_model.save(model, os.path.join(output, \"1\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking which tweets were misclassified (manual sanity check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "7NPaoToXnuts",
    "outputId": "f4507b8b-f508-4175-ada9-f04d02cf5cf2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 0.76422673 0.0 @Itzike ????  ??? ???\n",
      "24 0.3598383 1.0 Ever since I saw &quot;27 dresses&quot; can't get this song out of my head.   â« http://blip.fm/~7h4d1\n",
      "25 0.95532244 0.0 Grill in the oven is almost as good as on a grill. But I have no yard to grill in \n",
      "34 0.2646958 1.0 Reading reviews.. One big flaw, only 8GB of storage on the palm. LAF!! Roll on iPhone release - 32GB on board. Defo not iPhone killer.  X\n",
      "37 0.51759243 0.0 thinking of you.. \n",
      "57 0.17786157 1.0 @ovi_sirb oops  lasÄ cÄ n-am atÃ¢Å£ia followeri\n",
      "63 0.2892712 1.0 I am so bored, someone entertain me? \n",
      "66 0.49493372 1.0 7:11 am first feeding! Caden was hungry from the long trip!  #fb\n",
      "70 0.8171786 0.0 @CourtneySit Hey are you quiet today or is it me...work was nuts today...no tweeting time \n",
      "73 0.680059 0.0 @katebuckjr what? My camera  okay fine u can get away with it, just be nice!\n",
      "78 0.72411424 0.0 No more ice \n",
      "80 0.67700213 0.0 Why isn't my picture showing! Hmmm - maybe it's cos I've got a face for radio and not for twitter! \n",
      "116 0.16455398 1.0 I want you to be here  Love youuu:-*\n",
      "117 0.26916614 1.0 wow the things that could happen to LeBRON James. SMH. lol. \n",
      "120 0.28851232 1.0 @grintoul Can't you run the IE6update code yourself??? That can't be dodgy: it's open source and you can run it yourself  Right?\n",
      "125 0.046333984 1.0 m off... studying again \n",
      "137 0.50109255 0.0 i got a shocking news in early morning. My beby got heart atack.. \n",
      "142 0.4919378 1.0 @brianso0syck cool  o really? Lady gaga made elevator.Timbo was in DD.cool as if true.but as Nic said b4:&quot;wikipedia don't kno wiki nothin&quot;\n",
      "144 0.20443545 1.0 Still resisting catching up on twats, need to NOT know the setlist  Superexcited.com!!!\n",
      "146 0.6160814 0.0 Off to a mtg with a florist and then hopping the train out to east hampton.  can't wait for all the rain to rtn this weekend \n",
      "147 0.47216776 1.0 Ishmael Beah is at my school today! if you haven't read his book A Long Way Gone you should! \n",
      "149 0.47228208 1.0 First day of finals and I don't have any \n",
      "152 0.52002203 0.0 @kirawr OMG..and you didnt tell me this before..Hhmm?!?! I cant see not zombie siching on you tho  bwahahaha i love you\n",
      "155 0.48851934 1.0 @CallMeRideOut awww *mauh* i will forever support u. Im now working on being ur entertainment lawyer/DJ \n",
      "158 0.9609743 0.0 @JenniferStirlin 5 yass!!!! lol, no one cares to follow me \n",
      "159 0.62976366 0.0 @SueB_ yup u can use 100APIs in an hour. the thing is that is exactly the reason y i hate twitter clients!they always go over that! \n",
      "169 0.6355145 0.0 looking tho hair styles, getting mine cut really really short  o well, it's for a good cause\n",
      "173 0.45814207 1.0 uhuiii... move up by 3  84. businessweek.com\n",
      "188 0.50081307 0.0 R.I.P Nathan and Chelsea  You were so young \n",
      "190 0.49513718 1.0 @nadinee its an old 1950s one with drawers and scratches and I love it \n",
      "196 0.7418729 0.0 nadia bit me on the arm \n",
      "197 0.43037513 1.0 @coldplay http://twitpic.com/5no6s - LRLRL is absolutely fantastic Guy!! I really hope i can get a physical copy, I got two chances   ...\n",
      "214 0.30712864 1.0 @SarahRouse I ain't from da US and i still think Flawless shuda won, diversity weren't realy dancing \n",
      "217 0.24949011 1.0 yay! i started blogging. no longer is my link in my profile directing people just to my facebook page \n",
      "218 0.036921732 1.0 woken up v early by a big pair of brown eyes. unfortunately they don't belong to a puppy...   but they do belong to an evil mayan \n",
      "224 0.21758725 1.0 @luckyluckster lol so we are quoting tupac now? i am not mad at it \n",
      "232 0.5652111 0.0 @hye_jin i'm actually back at tcnj taking classes  omgsh you're in belize?! so lucky!! how long are you there for?? oo what are the NCLEX?\n",
      "242 0.5510375 0.0 @JessObsess Yeah, just random body parts. Like the tip of a wing, a rat tail, a crawdad head, mice heads....it was bad. \n",
      "244 0.21589482 1.0 @v18rocks MAGIC. only hope left. \n",
      "246 0.73566335 0.0 It's so much easier to say you'll take responsibility for your actions when you don't think you have to. \n",
      "251 0.51846737 0.0 @hollygable aww brilliant holly. day has been good but your pops is being a git said my monster drawing was crap and fudge's was brill \n",
      "255 0.83270645 0.0 @LaurelEdelstein  its almost your weekend, loverpants. Wuuuuv you! (And think its totally acceptable to twitter at soupplantation)\n",
      "256 0.101897396 1.0 Ahhhh. I missed seeing JB on sportscenter.  &amp; Happy saturday!!!  Mariah (:\n",
      "260 0.041260764 1.0 @zaaik ...I'm sorry. But palm pre still has many problems \n",
      "267 0.53478116 0.0 @katernz Aww no  I got two bonus tickets for next week hehe! Bring on the 30mil!\n",
      "268 0.20536628 1.0 @timshi Yes. Yes, it would be REALLY cool. I wish I had the capital.  \n",
      "269 0.39480945 1.0 getting a month off school for photos shoots over in LA! yay! i leave in 2 days  xx\n",
      "273 0.20024173 1.0 @BIG_TONE no reason probably just missed u will do so right now \n",
      "274 0.27160758 1.0 @petewentz I want to see your look now \n",
      "277 0.452459 1.0 I'm an Art director but don't send me on a photo shoot? The match begins... \n",
      "285 0.7058248 0.0 Random tip of the day: don't bring the pretty little lemon cakes from Costco to anywhere. No one will eat them. Although they are good \n",
      "286 0.56228155 0.0 @Markl_Sparkle Thanks! Lets hope she comes... shes not returning my calls or texts!! \n",
      "291 0.34780753 1.0 @chloelisabeth I just realized that everyone I know is pretty much a couple. Huge fail!! Go go gadget single life \n",
      "298 0.48390257 1.0 the last time we seen each other being last nite  haha\n",
      "299 0.059071776 1.0 @DanWarp No! I don't want dirty teeth. Besides, I lived for 14 years without a billion dollars. \n",
      "316 0.13234687 1.0 @godfreychan seems working now \n",
      "319 0.15827681 1.0 I got almost everything I need for NYC, and now I'm babysitting \n",
      "329 0.022984447 1.0 Can't believe I bashed thru the heavy rain n reached kranji on time! Now waiting for Shin. Pray for journey mercy!  - http://tweet.sg\n",
      "340 0.1142511 1.0 @Zombie_Claire I don't want to b motivated. I like being lazy, angry, jaded &amp; disgruntled. \n",
      "343 0.27208215 1.0 Just finished a drawing.  I'll have to show you later.  It's a present, so I can't post it yet \n",
      "348 0.18162695 1.0 The key to my bicycle lock broke as I tried to unlock it to lock it to a rail... oh well, God will provide me with a better lock \n",
      "355 0.95870405 0.0 Listening to the new music and then going to watch Will &amp; Grace which always makes me laugh lol...Ugh i have to wake up early tomorrow! \n",
      "356 0.37825137 1.0 time for a walk. it was raining this am. see you after maintenance.  keep yourself surrounded by people who love Him without excuse!\n",
      "359 0.12947215 1.0 Orlando up on Cleveland BIG in the 1st Half - they're showing they want to win the East and go to the Finals ... uh oh Lebron  #NBA\n",
      "369 0.27188203 1.0 @KellydonMorton who woulda thunk it \n",
      "373 0.75538665 0.0 Goin to the gym   i hate it lol.  Gotta keep these curves under control hehe\n",
      "378 0.24464075 1.0 @MariahCarey so glitter's gonna be screened on friday here in SA and i cant wait...you were great in it mimi \n",
      "380 0.872716 0.0 @binko101   its just a picture lol. i was excited for a video. oh well. hhahha\n",
      "381 0.61497396 0.0 Attn: musicmama, Media Man, &amp; timd1011: :mad: I Declare A Rematch!!!!     I didn't get my message about playin.. http://tinyurl.com/m8yxoj\n",
      "385 0.9870658 0.0 @israeliwine we made a lamb maqluba .. . a lamb leg with eggplants, cauliflower, tomatoes, onions, spices and rice. mmm as for wine- \n",
      "390 0.8930833 0.0 @Gaminegirlie But I love my right hand \n",
      "391 0.4679652 1.0 @aishaladon Cool!  Right now, I'm trying to figure out why I got your tweet on my phone, but not on my web twitter...\n",
      "410 0.9321294 0.0 @mileycyrus i heaaard about youu on the radio. and the twitter latest  x\n",
      "418 0.9504233 0.0 Pachas was a animal house last night!!! Get well soon homie!!! \n",
      "422 0.597481 0.0 I am a gregarious being, I love connecting with people...  Too bad very few people understand that.\n",
      "424 0.9066272 0.0 @mileycyrus http://twitpic.com/7vvww - With this you look cheap \n",
      "429 0.49820912 1.0 where omar? Lol, i'm lost. GOodnight everyone. \n",
      "432 0.24123035 1.0 Off to bed. Work tomorrow 8:45 to 4:15. Then sleeeeepinn all the rest of sunday. \n",
      "435 0.66330224 0.0 I can't support see these photos, I know that u're very friends, and i love all, but I can't support see nicholas with anybody \n",
      "436 0.06992739 1.0 Big day for @SES_Toronto - big big day... All of a sudden the massive Chicago delay doesn't hurt as much \n",
      "448 0.3582869 1.0 @iMariela where did you go? \n",
      "449 0.85420907 0.0 I must be the biggest dork in the complex.  The cool kids were smokin' ad I didn't get asked to stay. \n",
      "470 0.15458925 1.0 Okay now that i will not be able to sleep tonight what should we do \n",
      "472 0.14790404 1.0 @GeminiTwisted  he was mortified that I took it so I told him I'd DM it. \n",
      "479 0.5148821 0.0 @DC_Princess_202 u HTK...pwease... \n",
      "480 0.88442653 0.0 Recovery  From A Night Out!\n",
      "482 0.32964876 1.0 @silkehartung i have a meeting with darrin or i'D be at home. Pub meetings rock though \n",
      "488 0.6349458 0.0 _@SirPsychoSexy I ain't got my mum anything &gt;.&lt; I forgoted! Bugerr I was in town yesterday too  ahh sexy no, homo yes XD\n",
      "493 0.6863207 0.0 @SARALOVESNICHOL Never ever change  You're so amazing ... Jonas for life ! Message me ok ?\n",
      "495 0.8020707 0.0 @jhgrant good plan, but: \n",
      "90  wrong out of  500 0.18 % misclassification\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "def load_json_file(json_path):\n",
    "    tweets = []\n",
    "    features = []\n",
    "    labels = []\n",
    "\n",
    "    with open(json_path, \"r\", encoding='latin-1') as file:\n",
    "\n",
    "        for line in file:\n",
    "\n",
    "            entry = json.loads(line)\n",
    "\n",
    "            if len(entry[\"features\"]) != 100:\n",
    "                raise ValueError(\n",
    "                    \"The size of the features of the entry with twitterid {} was not expected\".format(\n",
    "                        entry[\"twitterid\"]))\n",
    "            tweets.append(entry[\"tweet\"])\n",
    "            labels.append(entry[\"sentiment\"] / 4)\n",
    "            features.append(entry[\"features\"])\n",
    "    return features, labels, tweets\n",
    "\n",
    "def create_dataset(directory):\n",
    "    all_files = os.listdir(directory)\n",
    "\n",
    "    all_tweets = []\n",
    "    all_features = []\n",
    "    all_labels = []\n",
    "\n",
    "    for file in all_files:\n",
    "        features, labels, tweets = load_json_file(os.path.join(directory, file))\n",
    "        all_features += features\n",
    "        all_labels += labels\n",
    "        all_tweets += tweets\n",
    "    all_features = np.array(all_features)\n",
    "    all_labels = np.array(all_labels)\n",
    "\n",
    "    return all_features, all_labels, all_tweets\n",
    "\n",
    "# x_train, y_train, tweets_train = create_dataset('./model_training/data/train/')\n",
    "# x_val, y_val, tweets_val = create_dataset('./model_training/data/dev/')\n",
    "x_eval, y_eval, tweets_eval = create_dataset('./model_training/data/eval/')\n",
    "\n",
    "\n",
    "wrong = 0\n",
    "total = 0\n",
    "for i in range(0, 500):\n",
    "  if( (model.predict(np.array([x_eval[i]]))[0][0] >= 0.5) != bool(y_eval[i]) ):\n",
    "    print(i+1, model.predict(np.array([x_eval[i]]))[0][0],y_eval[i], tweets_eval[i])\n",
    "    wrong+=1\n",
    "  total+=1\n",
    "\n",
    "print(wrong, ' wrong out of ', total, wrong/total, '% misclassification')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "X2n3TFavuNtV"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "2 epochs_deep_cnn.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
